{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "# Input paths\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/train/images\"\n",
    "mask_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/train/masks\"\n",
    "\n",
    "# Output paths\n",
    "out_image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/images\"\n",
    "out_mask_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/masks\"\n",
    "\n",
    "os.makedirs(out_image_dir, exist_ok=True)\n",
    "os.makedirs(out_mask_dir, exist_ok=True)\n",
    "\n",
    "# Supported extensions\n",
    "exts = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
    "\n",
    "image_files = []\n",
    "for ext in exts:\n",
    "    image_files.extend(glob(os.path.join(image_dir, ext)))\n",
    "\n",
    "print(f\"Found {len(image_files)} images.\")\n",
    "\n",
    "for img_path in image_files:\n",
    "    fname = os.path.basename(img_path)\n",
    "    name, ext = os.path.splitext(fname)\n",
    "\n",
    "    # Load image and mask\n",
    "    mask_path = os.path.join(mask_dir, fname)\n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"Mask not found for {fname}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    mask = Image.open(mask_path)\n",
    "\n",
    "    # Save original\n",
    "    img.save(os.path.join(out_image_dir, fname))\n",
    "    mask.save(os.path.join(out_mask_dir, fname))\n",
    "\n",
    "    # Horizontal flip\n",
    "    img_h = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    mask_h = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    img_h.save(os.path.join(out_image_dir, f\"{name}_hflip{ext}\"))\n",
    "    mask_h.save(os.path.join(out_mask_dir, f\"{name}_hflip{ext}\"))\n",
    "\n",
    "    # Vertical flip\n",
    "    img_v = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    mask_v = mask.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    img_v.save(os.path.join(out_image_dir, f\"{name}_vflip{ext}\"))\n",
    "    mask_v.save(os.path.join(out_mask_dir, f\"{name}_vflip{ext}\"))\n",
    "\n",
    "print(\"Augmentation completed. Files saved in:\")\n",
    "print(out_image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Features for SAM \n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/images\"  \n",
    "sam_output_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_train_features\"\n",
    "os.makedirs(sam_output_dir, exist_ok=True)\n",
    "\n",
    "# Load SAM model for prediction\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# === Load image paths ===\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, fname)\n",
    "    for fname in os.listdir(image_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "# === Inference loop ===\n",
    "for path in tqdm(image_paths, desc=\"Extracting SAM masks\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    inputs = sam_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = sam_model(**inputs)\n",
    "        masks = outputs.pred_masks  # [B, num_masks, H, W], logits\n",
    "        iou_scores = outputs.iou_scores  # [B, num_masks]\n",
    "\n",
    "        # Pick best mask per image \n",
    "        best_mask_idx = iou_scores.argmax(dim=1)[0]  # scalar\n",
    "        best_mask_logits = masks[0, best_mask_idx]    # [H, W], logits\n",
    "\n",
    "        # Convert logits to soft mask \n",
    "        soft_mask = torch.sigmoid(best_mask_logits).cpu()\n",
    "\n",
    "    # Save soft mask \n",
    "    torch.save(soft_mask, os.path.join(sam_output_dir, f\"{base_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip install transformers==4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepak1010/anaconda3/envs/sam-vit-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/deepak1010/anaconda3/envs/sam-vit-env/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/deepak1010/anaconda3/envs/sam-vit-env/lib/python3.9/site-packages/torch/cuda/__init__.py:132: UserWarning: \n",
      "    Found GPU1 NVIDIA GeForce GT 710 which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))\n",
      "Extracting DINOv2 features: 100%|███████████████████████| 5800/5800 [06:50<00:00, 14.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generate Features for DINOv2 \n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/images\"  \n",
    "dino_output_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_train_features\"\n",
    "os.makedirs(dino_output_dir, exist_ok=True)\n",
    "\n",
    "# Load DINOv2 ViT model and processor \n",
    "model = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "# Load image paths\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, fname)\n",
    "    for fname in os.listdir(image_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "# Inference loop\n",
    "for path in tqdm(image_paths, desc=\"Extracting DINOv2 features\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state[0]  # [num_tokens, C]\n",
    "\n",
    "        # Remove CLS token\n",
    "        spatial_tokens = last_hidden_state[1:]  # [num_patches, C]\n",
    "        C = spatial_tokens.shape[-1]\n",
    "        num_patches = spatial_tokens.shape[0]\n",
    "        H = W = int(num_patches ** 0.5)\n",
    "\n",
    "        if H * W != num_patches:\n",
    "            raise ValueError(f\"Expected square feature map but got {num_patches} tokens.\")\n",
    "\n",
    "        features = spatial_tokens.reshape(H, W, C).permute(2, 0, 1).contiguous()  # [C, H, W]\n",
    "\n",
    "    # Save feature map\n",
    "    torch.save(features.cpu(), os.path.join(dino_output_dir, f\"{base_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.22\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SAM masks: 100%|███████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generate Features for SAM for test\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_kvasir/images\"  # <-- update this\n",
    "sam_output_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_kvasir_test_features\"\n",
    "os.makedirs(sam_output_dir, exist_ok=True)\n",
    "\n",
    "# Load SAM model for mask prediction \n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# === Load image paths ===\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, fname)\n",
    "    for fname in os.listdir(image_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "# === Inference loop ===\n",
    "for path in tqdm(image_paths, desc=\"Extracting SAM masks\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    inputs = sam_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = sam_model(**inputs)\n",
    "        masks = outputs.pred_masks  # [B, num_masks, H, W], logits\n",
    "        iou_scores = outputs.iou_scores  # [B, num_masks]\n",
    "\n",
    "        # Pick best mask per image \n",
    "        best_mask_idx = iou_scores.argmax(dim=1)[0]  # scalar\n",
    "        best_mask_logits = masks[0, best_mask_idx]    # [H, W], logits\n",
    "\n",
    "        # Convert logits to soft mask\n",
    "        soft_mask = torch.sigmoid(best_mask_logits).cpu()\n",
    "\n",
    "    # Save soft mask tensor\n",
    "    torch.save(soft_mask, os.path.join(sam_output_dir, f\"{base_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SAM masks: 100%|█████████████████████████████████| 62/62 [00:49<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generate Features for SAM \n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_cvc/images\"  # <-- update this\n",
    "sam_output_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_cvc_test_features\"\n",
    "os.makedirs(sam_output_dir, exist_ok=True)\n",
    "\n",
    "# Load SAM model for mask prediction instead of just embeddings\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# === Load image paths ===\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, fname)\n",
    "    for fname in os.listdir(image_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "# === Inference loop ===\n",
    "for path in tqdm(image_paths, desc=\"Extracting SAM masks\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    inputs = sam_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = sam_model(**inputs)\n",
    "        masks = outputs.pred_masks  # [B, num_masks, H, W], logits\n",
    "        iou_scores = outputs.iou_scores  # [B, num_masks]\n",
    "\n",
    "        # Pick best mask per image (assuming batch size = 1)\n",
    "        best_mask_idx = iou_scores.argmax(dim=1)[0]  # scalar\n",
    "        best_mask_logits = masks[0, best_mask_idx]    # [H, W], logits\n",
    "\n",
    "        # Convert logits to soft mask (sigmoid probabilities)\n",
    "        soft_mask = torch.sigmoid(best_mask_logits).cpu()\n",
    "\n",
    "    # Save soft mask tensor\n",
    "    torch.save(soft_mask, os.path.join(sam_output_dir, f\"{base_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting DINOv2 features: 100%|█████████████████████████| 100/100 [00:04<00:00, 23.44it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generate Dinov2 Features for test\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_kvasir/images\"  # Same image dir\n",
    "dino_output_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_kvasir_test_features\"\n",
    "os.makedirs(dino_output_dir, exist_ok=True)\n",
    "\n",
    "# Load DINOv2 ViT model and processor\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "# === Load image paths ===\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, fname)\n",
    "    for fname in os.listdir(image_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "# === Inference loop ===\n",
    "for path in tqdm(image_paths, desc=\"Extracting DINOv2 features\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state[0]  # [num_tokens, C]\n",
    "        \n",
    "        # Remove CLS token\n",
    "        spatial_tokens = last_hidden_state[1:]  # [num_patches, C]\n",
    "        C = spatial_tokens.shape[-1]\n",
    "        num_patches = spatial_tokens.shape[0]\n",
    "        H = W = int(num_patches ** 0.5)\n",
    "    \n",
    "        if H * W != num_patches:\n",
    "            raise ValueError(f\"Expected square feature map but got {num_patches} tokens.\")\n",
    "    \n",
    "        features = spatial_tokens.reshape(H, W, C).permute(2, 0, 1).contiguous()  # [C, H, W]\n",
    "\n",
    "    # Save feature map\n",
    "    torch.save(features.cpu(), os.path.join(dino_output_dir, f\"{base_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting DINOv2 features: 100%|███████████████████████████| 62/62 [00:01<00:00, 32.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generate Dinov2 Features for test\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_cvc/images\"  # Same image dir\n",
    "dino_output_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_cvc_test_features\"\n",
    "os.makedirs(dino_output_dir, exist_ok=True)\n",
    "\n",
    "# Load DINOv2 ViT model and processor\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "# === Load image paths ===\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, fname)\n",
    "    for fname in os.listdir(image_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "# === Inference loop ===\n",
    "for path in tqdm(image_paths, desc=\"Extracting DINOv2 features\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state[0]  # [num_tokens, C]\n",
    "        \n",
    "        # Remove CLS token\n",
    "        spatial_tokens = last_hidden_state[1:]  # [num_patches, C]\n",
    "        C = spatial_tokens.shape[-1]\n",
    "        num_patches = spatial_tokens.shape[0]\n",
    "        H = W = int(num_patches ** 0.5)\n",
    "    \n",
    "        if H * W != num_patches:\n",
    "            raise ValueError(f\"Expected square feature map but got {num_patches} tokens.\")\n",
    "    \n",
    "        features = spatial_tokens.reshape(H, W, C).permute(2, 0, 1).contiguous()  # [C, H, W]\n",
    "\n",
    "    # Save feature map\n",
    "    torch.save(features.cpu(), os.path.join(dino_output_dir, f\"{base_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In similar manner generate test features for unseen dataset (cvc-300, etis, cvc-colondb) for SAM and DINOv2.............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate OneFormer Features\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_dir = \"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/images\"  # update path\n",
    "oneformer_output_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/oneformer_train_features\"\n",
    "os.makedirs(oneformer_output_dir, exist_ok=True)\n",
    "\n",
    "# Load OneFormer (COCO semantic segmentation variant)\n",
    "processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_coco_swin_large\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_coco_swin_large\").to(device)\n",
    "\n",
    "task_type = \"semantic\"  # can also be \"instance\" or \"panoptic\"\n",
    "\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, fname)\n",
    "    for fname in os.listdir(image_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "])\n",
    "\n",
    "for path in tqdm(image_paths, desc=\"Extracting OneFormer dense features\"):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, task_inputs=[task_type], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Dense spatial feature map \n",
    "    dense_features = outputs.pixel_decoder_hidden_states[-1]  # shape: [1, C, H, W]\n",
    "    dense_features = dense_features.squeeze(0).cpu()  # remove batch dim\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(path))[0]\n",
    "    torch.save(dense_features, os.path.join(oneformer_output_dir, f\"{base_name}.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarly generate all the test features for Oneformer.....................\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([1, 1, 352, 352])\n",
      "Distillation1: 0.06858216971158981\n",
      "Distillation2: 0.06587187945842743\n"
     ]
    }
   ],
   "source": [
    "#Unet++ 3F \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "\n",
    "__all__ = ['VGGBlock', 'UNet', 'NestedUNet']\n",
    "\n",
    "\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class NestedUNet(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, deep_supervision=False,\n",
    "                 f1_channels=9, f2_channels=768, f3_channels=256):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # --- Standard UNet++ blocks ---\n",
    "        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n",
    "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
    "        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n",
    "\n",
    "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "\n",
    "        # --- Latent space projection layers (L1 / L2) ---\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Conv2d(nb_filter[4], 256, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear2 = nn.Conv2d(256, 256, 1)\n",
    "\n",
    "        # --- Fusion layer for external features ---\n",
    "        in_channels_total = f1_channels + f2_channels + f3_channels\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Conv2d(in_channels_total, 256, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, input, f1, f2, f3):\n",
    "        # --- Encoder path ---\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))  # bottleneck\n",
    "\n",
    "        # --- Latent projections ---\n",
    "        L1 = self.linear1(x4_0)\n",
    "        L2 = self.linear2(L1)\n",
    "\n",
    "        # --- Feature alignment & fusion ---\n",
    "        target_size = f1.shape[2:]\n",
    "        if f2.shape[2:] != target_size:\n",
    "            f2 = F.interpolate(f2, size=target_size, mode='bilinear', align_corners=False)\n",
    "        if f3.shape[2:] != target_size:\n",
    "            f3 = F.interpolate(f3, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        f = torch.cat([f1, f2, f3], dim=1)\n",
    "        f = self.linear(f)  # -> (B,256,H,W)\n",
    "\n",
    "        # --- Frequency filtering ---\n",
    "        f_freq = torch.fft.fft2(f, norm=\"ortho\")\n",
    "        f_freq_shifted = torch.fft.fftshift(f_freq)\n",
    "\n",
    "        B, C, H, W = f_freq_shifted.shape\n",
    "        low_mask = torch.zeros_like(f_freq_shifted)\n",
    "        high_mask = torch.ones_like(f_freq_shifted)\n",
    "\n",
    "        center_h = H // 2\n",
    "        center_w = W // 2\n",
    "        radius = min(H, W) // 6\n",
    "\n",
    "        low_mask[:, :, center_h-radius:center_h+radius, center_w-radius:center_w+radius] = 1\n",
    "        high_mask = 1 - low_mask\n",
    "\n",
    "        f_low = torch.real(torch.fft.ifft2(torch.fft.ifftshift(f_freq_shifted * low_mask), norm=\"ortho\"))\n",
    "        f_high = torch.real(torch.fft.ifft2(torch.fft.ifftshift(f_freq_shifted * high_mask), norm=\"ortho\"))\n",
    "\n",
    "        if f_low.shape[2:] != L1.shape[2:]:\n",
    "            f_low = F.interpolate(f_low, size=L1.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        if f_high.shape[2:] != L2.shape[2:]:\n",
    "            f_high = F.interpolate(f_high, size=L2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        distillation1 = F.mse_loss(f_low, L1)\n",
    "        distillation2 = F.mse_loss(f_high, L2)\n",
    "\n",
    "        # --- Decoder path ---\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            output1 = self.final1(x0_1)\n",
    "            output2 = self.final2(x0_2)\n",
    "            output3 = self.final3(x0_3)\n",
    "            output4 = self.final4(x0_4)\n",
    "            return [output1, output2, output3, output4], distillation1, distillation2\n",
    "        else:\n",
    "            output = self.final(x0_4)\n",
    "            return output, distillation1, distillation2\n",
    "\n",
    "\n",
    "# --- Quick test ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = NestedUNet(num_classes=1, input_channels=3).to(device)\n",
    "    x = torch.randn(1, 3, 352, 352).to(device)\n",
    "    f1 = torch.randn(1, 9, 32, 32).to(device)       # SAM\n",
    "    f2 = torch.randn(1, 768, 32, 32).to(device)     # DINOv2\n",
    "    f3 = torch.randn(1, 256, 32, 32).to(device)     # OneFormer\n",
    "\n",
    "    out, d1, d2 = model(x, f1, f2, f3)\n",
    "    print(\"Output:\", out.shape)\n",
    "    print(\"Distillation1:\", d1.item())\n",
    "    print(\"Distillation2:\", d2.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class UNetPPWithSAMDINOneFormerDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, sam_feature_dir, dino_feature_dir, oneformer_feature_dir, transform=None, feature_size=(64, 64)):\n",
    "        \"\"\"\n",
    "        feature_size: tuple (H, W) → all SAM/DINO/OneFormer features will be resized to this shape\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.sam_feature_dir = sam_feature_dir\n",
    "        self.dino_feature_dir = dino_feature_dir\n",
    "        self.oneformer_feature_dir = oneformer_feature_dir\n",
    "        self.transform = transform\n",
    "        self.feature_size = feature_size\n",
    "        self.image_names = sorted(os.listdir(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        base_name, _ = os.path.splitext(image_name)\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "\n",
    "        # Find mask file\n",
    "        mask_path = None\n",
    "        for ext in [\".jpg\", \".png\"]:\n",
    "            candidate = os.path.join(self.mask_dir, base_name + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                mask_path = candidate\n",
    "                break\n",
    "        if mask_path is None:\n",
    "            raise FileNotFoundError(f\"No mask found for {base_name}\")\n",
    "\n",
    "        # Feature paths\n",
    "        f1_path = os.path.join(self.sam_feature_dir, base_name + \".pt\")        # SAM\n",
    "        f2_path = os.path.join(self.dino_feature_dir, base_name + \".pt\")       # DINO\n",
    "        f3_path = os.path.join(self.oneformer_feature_dir, base_name + \".pt\")  # OneFormer\n",
    "\n",
    "        # Load data\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        f1 = torch.load(f1_path)  # SAM features [N, C, H, W] or [C, H, W]\n",
    "        f2 = torch.load(f2_path)  # DINO features\n",
    "        f3 = torch.load(f3_path)  # OneFormer features\n",
    "\n",
    "        # Ensure 4D shape for interpolation\n",
    "        if f1.dim() == 3:\n",
    "            f1 = f1.unsqueeze(0)  # [1, C, H, W]\n",
    "        if f2.dim() == 3:\n",
    "            f2 = f2.unsqueeze(0)\n",
    "        if f3.dim() == 3:\n",
    "            f3 = f3.unsqueeze(0)\n",
    "\n",
    "        # Resize features to the same size\n",
    "        f1 = F.interpolate(f1, size=self.feature_size, mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        f2 = F.interpolate(f2, size=self.feature_size, mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        f3 = F.interpolate(f3, size=self.feature_size, mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "\n",
    "        # Apply transforms to image and mask\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=np.array(image), mask=np.array(mask))\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"ground_truth_mask\": mask.float() / 255.0,\n",
    "            \"f1\": f1,\n",
    "            \"f2\": f2,\n",
    "            \"f3\": f3,\n",
    "            \"image_name\": image_name\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450\n",
      "25\n",
      "16\n",
      "15\n",
      "49\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepak1010/anaconda3/envs/sam-vit-env/lib/python3.9/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# import albumentations as A\n",
    "# import os\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Transform for both train/valid\n",
    "# transform = A.Compose([\n",
    "#     A.Resize(352, 352),\n",
    "#     A.Normalize(),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# train_dataset = UNetWithSAMAndDINODataset(\n",
    "#     image_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/images\",\n",
    "#     mask_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/masks\",\n",
    "#     sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_train_features\",\n",
    "#     dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_train_features\",\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# test_kvasir_dataset = UNetWithSAMAndDINODataset(\n",
    "#     image_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_kvasir/images\",\n",
    "#     mask_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_kvasir/masks\",\n",
    "#     sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_kvasir_test_features\",\n",
    "#     dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_kvasir_test_features\",\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# test_cvc_dataset = UNetWithSAMAndDINODataset(\n",
    "#     image_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_cvc/images\",\n",
    "#     mask_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_cvc/masks\",\n",
    "#     sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_cvc_test_features\",\n",
    "#     dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_cvc_test_features\",\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# test_cvc_300_dataset = UNetWithSAMAndDINODataset(\n",
    "#     image_dir=\"/home/deepak1010/Shivanshu Code/CVC-300/images\",\n",
    "#     mask_dir=\"/home/deepak1010/Shivanshu Code/CVC-300/masks\",\n",
    "#     sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_cvc_300_test_features\",\n",
    "#     dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_cvc_300_test_features\",\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# test_etis_dataset = UNetWithSAMAndDINODataset(\n",
    "#     image_dir=\"/home/deepak1010/Shivanshu Code/ETIS/images\",\n",
    "#     mask_dir=\"/home/deepak1010/Shivanshu Code/ETIS/masks\",\n",
    "#     sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_etis_test_features\",\n",
    "#     dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_etis_test_features\",\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# test_cvc_colondb_dataset = UNetWithSAMAndDINODataset(\n",
    "#     image_dir=\"/home/deepak1010/Shivanshu Code/CVC-ColonDB/images\",\n",
    "#     mask_dir=\"/home/deepak1010/Shivanshu Code/CVC-ColonDB/masks\",\n",
    "#     sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_cvc_colondb_test_features\",\n",
    "#     dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_cvc_colondb_test_features\",\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# test_kvasir_dataloader = DataLoader(test_kvasir_dataset, batch_size=4, shuffle=False)\n",
    "# test_cvc_dataloader = DataLoader(test_cvc_dataset, batch_size=4, shuffle=False)\n",
    "# test_cvc_300_dataloader = DataLoader(test_cvc_300_dataset, batch_size=4, shuffle=False)\n",
    "# test_etis_dataloader = DataLoader(test_etis_dataset, batch_size=4, shuffle=False)\n",
    "# test_cvc_colondb_dataloader = DataLoader(test_cvc_colondb_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "# print(len(train_dataloader))\n",
    "# print(len(test_kvasir_dataloader))\n",
    "# print(len(test_cvc_dataloader))\n",
    "# print(len(test_cvc_300_dataloader))\n",
    "# print(len(test_etis_dataloader))\n",
    "# print(len(test_cvc_colondb_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967\n",
      "17\n",
      "11\n",
      "10\n",
      "33\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "import os\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transform for both train/valid\n",
    "transform = A.Compose([\n",
    "    A.Resize(352, 352),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_dataset = UNetPPWithSAMDINOneFormerDataset(\n",
    "    image_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/images\",\n",
    "    mask_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc_augmented/train/masks\",\n",
    "    sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_train_features\",\n",
    "    dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_train_features\",\n",
    "    oneformer_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/oneformer_train_features\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_kvasir_dataset = UNetPPWithSAMDINOneFormerDataset(\n",
    "    image_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_kvasir/images\",\n",
    "    mask_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_kvasir/masks\",\n",
    "    sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_kvasir_test_features\",\n",
    "    dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_kvasir_test_features\",\n",
    "    oneformer_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/oneformer_kvasir_test_features\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_cvc_dataset = UNetPPWithSAMDINOneFormerDataset(\n",
    "    image_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_cvc/images\",\n",
    "    mask_dir=\"/home/deepak1010/Shivanshu Code/mixed_ds_kvasir_cvc/test_cvc/masks\",\n",
    "    sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_cvc_test_features\",\n",
    "    dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_cvc_test_features\",\n",
    "    oneformer_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/oneformer_cvc_test_features\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_cvc_300_dataset = UNetPPWithSAMDINOneFormerDataset(\n",
    "    image_dir=\"/home/deepak1010/Shivanshu Code/CVC-300/images\",\n",
    "    mask_dir=\"/home/deepak1010/Shivanshu Code/CVC-300/masks\",\n",
    "    sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_cvc_300_test_features\",\n",
    "    dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_cvc_300_test_features\",\n",
    "    oneformer_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/oneformer_cvc_300_test_features\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_etis_dataset = UNetPPWithSAMDINOneFormerDataset(\n",
    "    image_dir=\"/home/deepak1010/Shivanshu Code/ETIS/images\",\n",
    "    mask_dir=\"/home/deepak1010/Shivanshu Code/ETIS/masks\",\n",
    "    sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_etis_test_features\",\n",
    "    dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_etis_test_features\",\n",
    "    oneformer_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/oneformer_etis_test_features\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_cvc_colondb_dataset = UNetPPWithSAMDINOneFormerDataset(\n",
    "    image_dir=\"/home/deepak1010/Shivanshu Code/CVC-ColonDB/images\",\n",
    "    mask_dir=\"/home/deepak1010/Shivanshu Code/CVC-ColonDB/masks\",\n",
    "    sam_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/sam_cvc_colondb_test_features\",\n",
    "    dino_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/dino_cvc_colondb_test_features\",\n",
    "    oneformer_feature_dir=\"/home/deepak1010/Shivanshu Code/features_sam_clip/oneformer_cvc_colondb_test_features\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "test_kvasir_dataloader = DataLoader(test_kvasir_dataset, batch_size=6, shuffle=False)\n",
    "test_cvc_dataloader = DataLoader(test_cvc_dataset, batch_size=6, shuffle=False)\n",
    "test_cvc_300_dataloader = DataLoader(test_cvc_300_dataset, batch_size=6, shuffle=False)\n",
    "test_etis_dataloader = DataLoader(test_etis_dataset, batch_size=6, shuffle=False)\n",
    "test_cvc_colondb_dataloader = DataLoader(test_cvc_colondb_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Quick check\n",
    "print(len(train_dataloader))\n",
    "print(len(test_kvasir_dataloader))\n",
    "print(len(test_cvc_dataloader))\n",
    "print(len(test_cvc_300_dataloader))\n",
    "print(len(test_etis_dataloader))\n",
    "print(len(test_cvc_colondb_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 0: 100%|█| 967/967 [24:56<00:00,  1.55s/it, Loss=0.834, Seg=0.834, Dist=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 000: Loss = 1.0143, Seg = 1.0143, Dist = 0.1807\n",
      "✅ Saved new best model at epoch 0 with train_loss = 1.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1: 100%|█| 967/967 [22:10<00:00,  1.38s/it, Loss=0.652, Seg=0.652, Dist=0.179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 001: Loss = 0.6308, Seg = 0.6308, Dist = 0.1778\n",
      "✅ Saved new best model at epoch 1 with train_loss = 0.6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2: 100%|████| 967/967 [22:00<00:00,  1.37s/it, Loss=0.28, Seg=0.28, Dist=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 002: Loss = 0.4594, Seg = 0.4594, Dist = 0.1770\n",
      "✅ Saved new best model at epoch 2 with train_loss = 0.4594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3: 100%|██| 967/967 [21:54<00:00,  1.36s/it, Loss=0.192, Seg=0.192, Dist=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 003: Loss = 0.3826, Seg = 0.3826, Dist = 0.1768\n",
      "✅ Saved new best model at epoch 3 with train_loss = 0.3826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 4: 100%|█| 967/967 [22:04<00:00,  1.37s/it, Loss=0.264, Seg=0.264, Dist=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 004: Loss = 0.3343, Seg = 0.3343, Dist = 0.1764\n",
      "✅ Saved new best model at epoch 4 with train_loss = 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 5: 100%|█| 967/967 [22:03<00:00,  1.37s/it, Loss=0.185, Seg=0.185, Dist=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 005: Loss = 0.2966, Seg = 0.2966, Dist = 0.1759\n",
      "✅ Saved new best model at epoch 5 with train_loss = 0.2966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 6: 100%|█| 967/967 [22:01<00:00,  1.37s/it, Loss=0.365, Seg=0.365, Dist=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 006: Loss = 0.2679, Seg = 0.2679, Dist = 0.1758\n",
      "✅ Saved new best model at epoch 6 with train_loss = 0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 7: 100%|█| 967/967 [22:09<00:00,  1.37s/it, Loss=0.471, Seg=0.471, Dist=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 007: Loss = 0.2424, Seg = 0.2424, Dist = 0.1756\n",
      "✅ Saved new best model at epoch 7 with train_loss = 0.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 8: 100%|█| 967/967 [22:08<00:00,  1.37s/it, Loss=0.231, Seg=0.231, Dist=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 008: Loss = 0.2201, Seg = 0.2201, Dist = 0.1754\n",
      "✅ Saved new best model at epoch 8 with train_loss = 0.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 9: 100%|█| 967/967 [22:24<00:00,  1.39s/it, Loss=0.187, Seg=0.187, Dist=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 009: Loss = 0.2017, Seg = 0.2017, Dist = 0.1754\n",
      "✅ Saved new best model at epoch 9 with train_loss = 0.2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 10: 100%|█| 967/967 [22:16<00:00,  1.38s/it, Loss=0.115, Seg=0.115, Dist=0.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 010: Loss = 0.1884, Seg = 0.1884, Dist = 0.1752\n",
      "✅ Saved new best model at epoch 10 with train_loss = 0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 11: 100%|█| 967/967 [22:22<00:00,  1.39s/it, Loss=0.333, Seg=0.333, Dist=0.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 011: Loss = 0.1724, Seg = 0.1724, Dist = 0.1752\n",
      "✅ Saved new best model at epoch 11 with train_loss = 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 12: 100%|█| 967/967 [22:22<00:00,  1.39s/it, Loss=0.159, Seg=0.159, Dist=0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 012: Loss = 0.1540, Seg = 0.1540, Dist = 0.1751\n",
      "✅ Saved new best model at epoch 12 with train_loss = 0.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 13: 100%|█| 967/967 [22:09<00:00,  1.38s/it, Loss=0.239, Seg=0.239, Dist=0.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 013: Loss = 0.1467, Seg = 0.1467, Dist = 0.1754\n",
      "✅ Saved new best model at epoch 13 with train_loss = 0.1467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 14: 100%|█| 967/967 [21:53<00:00,  1.36s/it, Loss=0.149, Seg=0.149, Dist=0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 014: Loss = 0.1380, Seg = 0.1380, Dist = 0.1754\n",
      "✅ Saved new best model at epoch 14 with train_loss = 0.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 15: 100%|█| 967/967 [21:48<00:00,  1.35s/it, Loss=0.153, Seg=0.153, Dist=0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 015: Loss = 0.1231, Seg = 0.1231, Dist = 0.1752\n",
      "✅ Saved new best model at epoch 15 with train_loss = 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 16: 100%|█| 967/967 [21:50<00:00,  1.36s/it, Loss=0.107, Seg=0.107, Dist=0.181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 016: Loss = 0.1160, Seg = 0.1160, Dist = 0.1750\n",
      "✅ Saved new best model at epoch 16 with train_loss = 0.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 17: 100%|█| 967/967 [21:58<00:00,  1.36s/it, Loss=0.0831, Seg=0.0831, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 017: Loss = 0.1099, Seg = 0.1099, Dist = 0.1752\n",
      "✅ Saved new best model at epoch 17 with train_loss = 0.1099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 18: 100%|█| 967/967 [21:51<00:00,  1.36s/it, Loss=0.0865, Seg=0.0865, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 018: Loss = 0.1021, Seg = 0.1021, Dist = 0.1753\n",
      "✅ Saved new best model at epoch 18 with train_loss = 0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 19: 100%|█| 967/967 [21:47<00:00,  1.35s/it, Loss=0.0735, Seg=0.0735, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 019: Loss = 0.1062, Seg = 0.1062, Dist = 0.1749\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 20: 100%|█| 967/967 [21:45<00:00,  1.35s/it, Loss=0.0607, Seg=0.0607, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 020: Loss = 0.0925, Seg = 0.0925, Dist = 0.1751\n",
      "✅ Saved new best model at epoch 20 with train_loss = 0.0925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 21: 100%|█| 967/967 [21:33<00:00,  1.34s/it, Loss=0.0932, Seg=0.0932, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 021: Loss = 0.0894, Seg = 0.0894, Dist = 0.1752\n",
      "✅ Saved new best model at epoch 21 with train_loss = 0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 22: 100%|█| 967/967 [21:27<00:00,  1.33s/it, Loss=0.0588, Seg=0.0588, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 022: Loss = 0.0832, Seg = 0.0832, Dist = 0.1751\n",
      "✅ Saved new best model at epoch 22 with train_loss = 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 23: 100%|█| 967/967 [21:27<00:00,  1.33s/it, Loss=0.0678, Seg=0.0678, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 023: Loss = 0.0805, Seg = 0.0805, Dist = 0.1751\n",
      "✅ Saved new best model at epoch 23 with train_loss = 0.0805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 24: 100%|█| 967/967 [21:14<00:00,  1.32s/it, Loss=0.0457, Seg=0.0457, Dist=0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 024: Loss = 0.0768, Seg = 0.0768, Dist = 0.1750\n",
      "✅ Saved new best model at epoch 24 with train_loss = 0.0768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 25: 100%|█| 967/967 [21:32<00:00,  1.34s/it, Loss=0.0344, Seg=0.0573, Dist=5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 025: Loss = 0.0384, Seg = 0.0632, Dist = 0.0024\n",
      "✅ Saved new best model at epoch 25 with train_loss = 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 26: 100%|█| 967/967 [21:42<00:00,  1.35s/it, Loss=0.042, Seg=0.0699, Dist=1.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 026: Loss = 0.0386, Seg = 0.0643, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 27: 100%|█| 967/967 [22:34<00:00,  1.40s/it, Loss=0.0368, Seg=0.0614, Dist=9.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 027: Loss = 0.0496, Seg = 0.0827, Dist = 0.0000\n",
      "Early stopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 28: 100%|█| 967/967 [22:57<00:00,  1.42s/it, Loss=0.0297, Seg=0.0495, Dist=5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 028: Loss = 0.0379, Seg = 0.0632, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 28 with train_loss = 0.0379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 29: 100%|█| 967/967 [23:01<00:00,  1.43s/it, Loss=0.0293, Seg=0.0488, Dist=1.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 029: Loss = 0.0423, Seg = 0.0706, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 30: 100%|█| 967/967 [23:02<00:00,  1.43s/it, Loss=0.0322, Seg=0.0537, Dist=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 030: Loss = 0.0368, Seg = 0.0614, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 30 with train_loss = 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 31: 100%|█| 967/967 [23:08<00:00,  1.44s/it, Loss=0.0452, Seg=0.0754, Dist=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 031: Loss = 0.0388, Seg = 0.0646, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 32: 100%|█| 967/967 [23:07<00:00,  1.43s/it, Loss=0.0425, Seg=0.0709, Dist=1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 032: Loss = 0.0448, Seg = 0.0747, Dist = 0.0000\n",
      "Early stopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 33: 100%|█| 967/967 [23:00<00:00,  1.43s/it, Loss=0.0365, Seg=0.0608, Dist=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 033: Loss = 0.0326, Seg = 0.0543, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 33 with train_loss = 0.0326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 34: 100%|█| 967/967 [23:00<00:00,  1.43s/it, Loss=0.0237, Seg=0.0396, Dist=1.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 034: Loss = 0.0303, Seg = 0.0505, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 34 with train_loss = 0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 35: 100%|█| 967/967 [22:59<00:00,  1.43s/it, Loss=0.0561, Seg=0.0935, Dist=1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 035: Loss = 0.0478, Seg = 0.0796, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 36: 100%|█| 967/967 [22:59<00:00,  1.43s/it, Loss=0.0279, Seg=0.0465, Dist=4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 036: Loss = 0.0303, Seg = 0.0505, Dist = 0.0000\n",
      "Early stopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 37: 100%|█| 967/967 [23:01<00:00,  1.43s/it, Loss=0.0407, Seg=0.0678, Dist=1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 037: Loss = 0.0353, Seg = 0.0589, Dist = 0.0000\n",
      "Early stopping counter: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 38: 100%|█| 967/967 [23:00<00:00,  1.43s/it, Loss=0.0249, Seg=0.0414, Dist=6.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 038: Loss = 0.0323, Seg = 0.0538, Dist = 0.0000\n",
      "Early stopping counter: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 39: 100%|█| 967/967 [23:00<00:00,  1.43s/it, Loss=0.0532, Seg=0.0887, Dist=5.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 039: Loss = 0.0313, Seg = 0.0522, Dist = 0.0000\n",
      "Early stopping counter: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 40: 100%|█| 967/967 [23:01<00:00,  1.43s/it, Loss=0.0325, Seg=0.0541, Dist=1.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 040: Loss = 0.0369, Seg = 0.0615, Dist = 0.0000\n",
      "Early stopping counter: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 41: 100%|█| 967/967 [22:57<00:00,  1.42s/it, Loss=0.0333, Seg=0.0556, Dist=1.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 041: Loss = 0.0258, Seg = 0.0429, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 41 with train_loss = 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 42: 100%|█| 967/967 [22:56<00:00,  1.42s/it, Loss=0.0286, Seg=0.0477, Dist=4.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 042: Loss = 0.0246, Seg = 0.0409, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 42 with train_loss = 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 43: 100%|█| 967/967 [22:54<00:00,  1.42s/it, Loss=0.0258, Seg=0.043, Dist=7.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 043: Loss = 0.0398, Seg = 0.0663, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 44: 100%|█| 967/967 [22:50<00:00,  1.42s/it, Loss=0.0198, Seg=0.0329, Dist=3.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 044: Loss = 0.0282, Seg = 0.0469, Dist = 0.0000\n",
      "Early stopping counter: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 45: 100%|█| 967/967 [22:52<00:00,  1.42s/it, Loss=0.0335, Seg=0.0558, Dist=6.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 045: Loss = 0.0270, Seg = 0.0449, Dist = 0.0000\n",
      "Early stopping counter: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 46: 100%|█| 967/967 [22:44<00:00,  1.41s/it, Loss=0.0263, Seg=0.0438, Dist=3.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 046: Loss = 0.0312, Seg = 0.0521, Dist = 0.0000\n",
      "Early stopping counter: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 47: 100%|█| 967/967 [22:44<00:00,  1.41s/it, Loss=0.0455, Seg=0.0759, Dist=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 047: Loss = 0.0270, Seg = 0.0450, Dist = 0.0000\n",
      "Early stopping counter: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 48: 100%|█| 967/967 [22:40<00:00,  1.41s/it, Loss=0.0217, Seg=0.0361, Dist=8.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 048: Loss = 0.0300, Seg = 0.0500, Dist = 0.0000\n",
      "Early stopping counter: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 49: 100%|█| 967/967 [22:38<00:00,  1.41s/it, Loss=0.0179, Seg=0.0298, Dist=3.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 049: Loss = 0.0222, Seg = 0.0370, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 49 with train_loss = 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 50: 100%|█| 967/967 [22:35<00:00,  1.40s/it, Loss=0.0334, Seg=0.0557, Dist=1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 050: Loss = 0.0215, Seg = 0.0359, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 50 with train_loss = 0.0215\n",
      "🔒 Freezing encoder layers (conv0_0 to conv4_0)...\n",
      "✅ Frozen: conv0_0.conv1.weight\n",
      "✅ Frozen: conv0_0.conv1.bias\n",
      "✅ Frozen: conv0_0.bn1.weight\n",
      "✅ Frozen: conv0_0.bn1.bias\n",
      "✅ Frozen: conv0_0.conv2.weight\n",
      "✅ Frozen: conv0_0.conv2.bias\n",
      "✅ Frozen: conv0_0.bn2.weight\n",
      "✅ Frozen: conv0_0.bn2.bias\n",
      "✅ Frozen: conv1_0.conv1.weight\n",
      "✅ Frozen: conv1_0.conv1.bias\n",
      "✅ Frozen: conv1_0.bn1.weight\n",
      "✅ Frozen: conv1_0.bn1.bias\n",
      "✅ Frozen: conv1_0.conv2.weight\n",
      "✅ Frozen: conv1_0.conv2.bias\n",
      "✅ Frozen: conv1_0.bn2.weight\n",
      "✅ Frozen: conv1_0.bn2.bias\n",
      "✅ Frozen: conv2_0.conv1.weight\n",
      "✅ Frozen: conv2_0.conv1.bias\n",
      "✅ Frozen: conv2_0.bn1.weight\n",
      "✅ Frozen: conv2_0.bn1.bias\n",
      "✅ Frozen: conv2_0.conv2.weight\n",
      "✅ Frozen: conv2_0.conv2.bias\n",
      "✅ Frozen: conv2_0.bn2.weight\n",
      "✅ Frozen: conv2_0.bn2.bias\n",
      "✅ Frozen: conv3_0.conv1.weight\n",
      "✅ Frozen: conv3_0.conv1.bias\n",
      "✅ Frozen: conv3_0.bn1.weight\n",
      "✅ Frozen: conv3_0.bn1.bias\n",
      "✅ Frozen: conv3_0.conv2.weight\n",
      "✅ Frozen: conv3_0.conv2.bias\n",
      "✅ Frozen: conv3_0.bn2.weight\n",
      "✅ Frozen: conv3_0.bn2.bias\n",
      "✅ Frozen: conv4_0.conv1.weight\n",
      "✅ Frozen: conv4_0.conv1.bias\n",
      "✅ Frozen: conv4_0.bn1.weight\n",
      "✅ Frozen: conv4_0.bn1.bias\n",
      "✅ Frozen: conv4_0.conv2.weight\n",
      "✅ Frozen: conv4_0.conv2.bias\n",
      "✅ Frozen: conv4_0.bn2.weight\n",
      "✅ Frozen: conv4_0.bn2.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 51: 100%|█| 967/967 [19:00<00:00,  1.18s/it, Loss=0.0187, Seg=0.0312, Dist=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 051: Loss = 0.0206, Seg = 0.0344, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 51 with train_loss = 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 52: 100%|█| 967/967 [19:03<00:00,  1.18s/it, Loss=0.0217, Seg=0.0361, Dist=7e-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 052: Loss = 0.0197, Seg = 0.0328, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 52 with train_loss = 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 53: 100%|█| 967/967 [19:01<00:00,  1.18s/it, Loss=0.0182, Seg=0.0304, Dist=4.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 053: Loss = 0.0194, Seg = 0.0323, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 53 with train_loss = 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 54: 100%|█| 967/967 [18:59<00:00,  1.18s/it, Loss=0.0172, Seg=0.0287, Dist=7.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 054: Loss = 0.0189, Seg = 0.0314, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 54 with train_loss = 0.0189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 55: 100%|█| 967/967 [18:59<00:00,  1.18s/it, Loss=0.0157, Seg=0.0261, Dist=5.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 055: Loss = 0.0185, Seg = 0.0309, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 55 with train_loss = 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 56: 100%|█| 967/967 [19:00<00:00,  1.18s/it, Loss=0.0183, Seg=0.0305, Dist=5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 056: Loss = 0.0182, Seg = 0.0303, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 56 with train_loss = 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 57: 100%|█| 967/967 [18:53<00:00,  1.17s/it, Loss=0.0144, Seg=0.024, Dist=4.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 057: Loss = 0.0179, Seg = 0.0298, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 57 with train_loss = 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 58: 100%|█| 967/967 [19:02<00:00,  1.18s/it, Loss=0.0195, Seg=0.0326, Dist=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 058: Loss = 0.0175, Seg = 0.0292, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 58 with train_loss = 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 59: 100%|█| 967/967 [18:58<00:00,  1.18s/it, Loss=0.0211, Seg=0.0351, Dist=9.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 059: Loss = 0.0174, Seg = 0.0289, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 59 with train_loss = 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 60: 100%|█| 967/967 [18:57<00:00,  1.18s/it, Loss=0.013, Seg=0.0217, Dist=5.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 060: Loss = 0.0170, Seg = 0.0283, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 60 with train_loss = 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 61: 100%|█| 967/967 [18:56<00:00,  1.18s/it, Loss=0.0149, Seg=0.0248, Dist=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 061: Loss = 0.0167, Seg = 0.0278, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 61 with train_loss = 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 62: 100%|█| 967/967 [18:52<00:00,  1.17s/it, Loss=0.0162, Seg=0.027, Dist=3.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 062: Loss = 0.0166, Seg = 0.0277, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 63: 100%|█| 967/967 [18:38<00:00,  1.16s/it, Loss=0.0171, Seg=0.0284, Dist=9.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 063: Loss = 0.0162, Seg = 0.0271, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 63 with train_loss = 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 64: 100%|█| 967/967 [17:36<00:00,  1.09s/it, Loss=0.0179, Seg=0.0298, Dist=6.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 064: Loss = 0.0160, Seg = 0.0266, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 64 with train_loss = 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 65: 100%|█| 967/967 [17:11<00:00,  1.07s/it, Loss=0.0135, Seg=0.0226, Dist=2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 065: Loss = 0.0158, Seg = 0.0264, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 65 with train_loss = 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 66: 100%|█| 967/967 [16:58<00:00,  1.05s/it, Loss=0.0152, Seg=0.0254, Dist=9.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 066: Loss = 0.0158, Seg = 0.0263, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 67: 100%|█| 967/967 [16:49<00:00,  1.04s/it, Loss=0.017, Seg=0.0283, Dist=5.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 067: Loss = 0.0154, Seg = 0.0257, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 67 with train_loss = 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 68: 100%|█| 967/967 [16:50<00:00,  1.05s/it, Loss=0.0165, Seg=0.0275, Dist=1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 068: Loss = 0.0152, Seg = 0.0253, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 68 with train_loss = 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 69: 100%|█| 967/967 [16:48<00:00,  1.04s/it, Loss=0.0158, Seg=0.0263, Dist=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 069: Loss = 0.0150, Seg = 0.0251, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 69 with train_loss = 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 70: 100%|█| 967/967 [16:47<00:00,  1.04s/it, Loss=0.018, Seg=0.03, Dist=1.53e-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 070: Loss = 0.0148, Seg = 0.0247, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 70 with train_loss = 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 71: 100%|█| 967/967 [16:54<00:00,  1.05s/it, Loss=0.0125, Seg=0.0208, Dist=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 071: Loss = 0.0146, Seg = 0.0243, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 71 with train_loss = 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 72: 100%|█| 967/967 [17:00<00:00,  1.06s/it, Loss=0.0137, Seg=0.0229, Dist=7.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 072: Loss = 0.0149, Seg = 0.0249, Dist = 0.0000\n",
      "Early stopping counter: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 73: 100%|█| 967/967 [17:01<00:00,  1.06s/it, Loss=0.0127, Seg=0.0211, Dist=4.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 073: Loss = 0.0142, Seg = 0.0237, Dist = 0.0000\n",
      "✅ Saved new best model at epoch 73 with train_loss = 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 74:   7%| | 70/967 [01:13<16:09,  1.08s/it, Loss=0.0144, Seg=0.0239, Dist=5.12"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from PIL import Image\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.clamp(min=1e-7, max=1 - 1e-7)\n",
    "        preds = preds.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        intersection = (preds * targets).sum(dim=(2, 3))\n",
    "        union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "# === Device, model, optimizer ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NestedUNet(num_classes=1, input_channels=3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "seg_loss = DiceLoss()\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# === Training settings ===\n",
    "num_epochs = 75\n",
    "best_train_loss = float(\"inf\")\n",
    "patience = 10\n",
    "early_stop_counter = 0\n",
    "min_delta = 1e-4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses, seg_losses, distil_losses = [], [], []\n",
    "\n",
    "    if epoch == 51:\n",
    "        print(\"Freezing encoder layers (conv0_0 to conv4_0)...\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(enc in name for enc in ['conv0_0', 'conv1_0', 'conv2_0', 'conv3_0', 'conv4_0']):\n",
    "                param.requires_grad = False\n",
    "                print(f\"Frozen: {name}\")\n",
    "\n",
    "    tq = tqdm(train_dataloader, desc=f\"[Train] Epoch {epoch}\")\n",
    "    for batch in tq:\n",
    "        x = batch[\"pixel_values\"].to(device)                # Input image\n",
    "        mask = batch[\"ground_truth_mask\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # SAM features\n",
    "        f1 = batch[\"f1\"].to(device)                         # Shape: [B, N, C, H, W]\n",
    "        B, N, C, H, W = f1.shape\n",
    "        f1 = f1.view(B, N * C, H, W)                         # → [B, 512, H, W] if N=1, C=512\n",
    "\n",
    "        # DINOv2 features\n",
    "        f2 = batch[\"f2\"].to(device)                         # Shape: [B, 512, H, W] \n",
    "        #Oneformer Features\n",
    "        f3 = batch[\"f3\"].to(device)\n",
    "        # print(f1.shape)\n",
    "        # print(f2.shape)\n",
    "        # print(f3.shape)\n",
    "        logits, dist1, dist2 = model(x, f1, f2, f3)\n",
    "\n",
    "        # Resize mask to match logits\n",
    "        mask_resized = nn.functional.interpolate(mask, size=logits.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        segmentation_loss = seg_loss(logits, mask_resized) + criteria(logits, mask_resized)\n",
    "\n",
    "        if epoch < 25:\n",
    "            loss = segmentation_loss\n",
    "        else:\n",
    "            loss = 0.6 * segmentation_loss + 0.1 * dist1 + 0.1 * dist2\n",
    "\n",
    "        if not torch.isnan(loss):\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "            seg_losses.append(segmentation_loss.item())\n",
    "            distil_losses.append((dist1.item() + dist2.item()) / 2)\n",
    "\n",
    "        tq.set_postfix({\n",
    "            \"Loss\": loss.item(),\n",
    "            \"Seg\": segmentation_loss.item(),\n",
    "            \"Dist\": (dist1.item() + dist2.item()) / 2\n",
    "        })\n",
    "\n",
    "    avg_train_loss = mean(epoch_losses)\n",
    "    print(f\"[Train] Epoch {epoch:03}: Loss = {avg_train_loss:.4f}, Seg = {mean(seg_losses):.4f}, Dist = {mean(distil_losses):.4f}\")\n",
    "\n",
    "    # === Early Stopping Check ===\n",
    "    if best_train_loss - avg_train_loss > min_delta:\n",
    "        best_train_loss = avg_train_loss\n",
    "        early_stop_counter = 0\n",
    "        save_path = \"/home/deepak1010/Shivanshu Code/features_sam_clip/unetplusplus_sam_dino_oneformer.pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"✅ Saved new best model at epoch {epoch} with train_loss = {best_train_loss:.4f}\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\" Early stopping triggered. Stopping training.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]: 100%|███████████████████████████████████████████████| 17/17 [00:20<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Results] Loss = 0.3000, Seg = 0.5000, Dist = 0.0000\n",
      "[Metrics] mDice = 0.8586, mIoU = 0.7584, Precision = 0.8831, Recall = 0.8467\n",
      "✅ Predictions saved in /home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_kvasir_unetplusplus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Testing for Kvasir with SAM + DINO + OneFormer Features ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from statistics import mean\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.clamp(min=1e-7, max=1 - 1e-7)\n",
    "        preds = preds.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        intersection = (preds * targets).sum(dim=(2, 3))\n",
    "        union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "def compute_metrics(preds, targets, smooth=1e-6):\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    tp = (preds * targets).sum().item()\n",
    "    fp = (preds * (1 - targets)).sum().item()\n",
    "    fn = ((1 - preds) * targets).sum().item()\n",
    "\n",
    "    dice = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
    "    iou = (tp + smooth) / (tp + fp + fn + smooth)\n",
    "    precision = (tp + smooth) / (tp + fp + smooth)\n",
    "    recall = (tp + smooth) / (tp + fn + smooth)\n",
    "\n",
    "    return dice, iou, precision, recall\n",
    "\n",
    "# ---- DEVICE ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- LOAD MODEL ----\n",
    "model = NestedUNet(num_classes=1, input_channels=3).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/home/deepak1010/Shivanshu Code/features_sam_clip/unetplusplus_sam_dino_oneformer.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "model.eval()\n",
    "\n",
    "# ---- LOSSES ----\n",
    "seg_loss = DiceLoss()\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---- TEST LOOP ----\n",
    "test_losses, test_seg_losses, test_distil_losses = [], [], []\n",
    "all_dice, all_iou, all_precision, all_recall = [], [], [], []\n",
    "\n",
    "save_pred_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_kvasir_unetplusplus\"\n",
    "os.makedirs(save_pred_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tq = tqdm(test_kvasir_dataloader, desc=\"[Test]\")\n",
    "    for i, batch in enumerate(tq):\n",
    "        # Input image & GT\n",
    "        x = batch[\"pixel_values\"].to(device)\n",
    "        mask = batch[\"ground_truth_mask\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # SAM features\n",
    "        f1 = batch[\"f1\"].to(device)                         # [B, N, C, H, W]\n",
    "        B, N, C, H, W = f1.shape\n",
    "        f1 = f1.view(B, N * C, H, W)                        # [B, N*C, H, W]\n",
    "\n",
    "        # DINOv2 features\n",
    "        f2 = batch[\"f2\"].to(device)                         # [B, 512, H, W]\n",
    "\n",
    "        # OneFormer features\n",
    "        f3 = batch[\"f3\"].to(device)                         # shape depends on extractor\n",
    "\n",
    "        # Forward pass\n",
    "        logits, dist1, dist2 = model(x, f1, f2, f3)\n",
    "\n",
    "        # Resize mask\n",
    "        mask_resized = nn.functional.interpolate(mask, size=logits.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Loss calculation (same as training)\n",
    "        seg_loss_value = seg_loss(logits, mask_resized) + criteria(logits, mask_resized)\n",
    "        loss = 0.6 * seg_loss_value + 0.1 * dist1 + 0.1 * dist2\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_seg_losses.append(seg_loss_value.item())\n",
    "        test_distil_losses.append((dist1.item() + dist2.item()) / 2)\n",
    "\n",
    "        # Predictions -> binary\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        # Metrics\n",
    "        dice, iou, precision, recall = compute_metrics(preds.cpu(), mask_resized.cpu())\n",
    "        all_dice.append(dice)\n",
    "        all_iou.append(iou)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "        # Save first 20 predictions\n",
    "        if i < 20:\n",
    "            save_image(preds, os.path.join(save_pred_dir, f\"pred_{i}.png\"))\n",
    "            save_image(mask_resized.float(), os.path.join(save_pred_dir, f\"gt_{i}.png\"))\n",
    "\n",
    "# ---- FINAL RESULTS ----\n",
    "print(f\"[Test Results] Loss = {mean(test_losses):.4f}, Seg = {mean(test_seg_losses):.4f}, Dist = {mean(test_distil_losses):.4f}\")\n",
    "print(f\"[Metrics] mDice = {mean(all_dice):.4f}, mIoU = {mean(all_iou):.4f}, Precision = {mean(all_precision):.4f}, Recall = {mean(all_recall):.4f}\")\n",
    "print(f\"✅ Predictions saved in {save_pred_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]: 100%|███████████████████████████████████████████████| 11/11 [00:12<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Results] Loss = 0.0684, Seg = 0.1139, Dist = 0.0000\n",
      "[Metrics] mDice = 0.9471, mIoU = 0.9000, Precision = 0.9563, Recall = 0.9387\n",
      "✅ Predictions saved in /home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_cvc_unetplusplus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Testing for Kvasir with SAM + DINO + OneFormer Features ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from statistics import mean\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.clamp(min=1e-7, max=1 - 1e-7)\n",
    "        preds = preds.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        intersection = (preds * targets).sum(dim=(2, 3))\n",
    "        union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "def compute_metrics(preds, targets, smooth=1e-6):\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    tp = (preds * targets).sum().item()\n",
    "    fp = (preds * (1 - targets)).sum().item()\n",
    "    fn = ((1 - preds) * targets).sum().item()\n",
    "\n",
    "    dice = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
    "    iou = (tp + smooth) / (tp + fp + fn + smooth)\n",
    "    precision = (tp + smooth) / (tp + fp + smooth)\n",
    "    recall = (tp + smooth) / (tp + fn + smooth)\n",
    "\n",
    "    return dice, iou, precision, recall\n",
    "\n",
    "# ---- DEVICE ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- LOAD MODEL ----\n",
    "model = NestedUNet(num_classes=1, input_channels=3).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/home/deepak1010/Shivanshu Code/features_sam_clip/unetplusplus_sam_dino_oneformer.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "model.eval()\n",
    "\n",
    "# ---- LOSSES ----\n",
    "seg_loss = DiceLoss()\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---- TEST LOOP ----\n",
    "test_losses, test_seg_losses, test_distil_losses = [], [], []\n",
    "all_dice, all_iou, all_precision, all_recall = [], [], [], []\n",
    "\n",
    "save_pred_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_cvc_unetplusplus\"\n",
    "os.makedirs(save_pred_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tq = tqdm(test_cvc_dataloader, desc=\"[Test]\")\n",
    "    for i, batch in enumerate(tq):\n",
    "        # Input image & GT\n",
    "        x = batch[\"pixel_values\"].to(device)\n",
    "        mask = batch[\"ground_truth_mask\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # SAM features\n",
    "        f1 = batch[\"f1\"].to(device)                         # [B, N, C, H, W]\n",
    "        B, N, C, H, W = f1.shape\n",
    "        f1 = f1.view(B, N * C, H, W)                        # [B, N*C, H, W]\n",
    "\n",
    "        # DINOv2 features\n",
    "        f2 = batch[\"f2\"].to(device)                         # [B, 512, H, W]\n",
    "\n",
    "        # OneFormer features\n",
    "        f3 = batch[\"f3\"].to(device)                         # shape depends on extractor\n",
    "\n",
    "        # Forward pass\n",
    "        logits, dist1, dist2 = model(x, f1, f2, f3)\n",
    "\n",
    "        # Resize mask\n",
    "        mask_resized = nn.functional.interpolate(mask, size=logits.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Loss calculation (same as training)\n",
    "        seg_loss_value = seg_loss(logits, mask_resized) + criteria(logits, mask_resized)\n",
    "        loss = 0.6 * seg_loss_value + 0.1 * dist1 + 0.1 * dist2\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_seg_losses.append(seg_loss_value.item())\n",
    "        test_distil_losses.append((dist1.item() + dist2.item()) / 2)\n",
    "\n",
    "        # Predictions -> binary\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        # Metrics\n",
    "        dice, iou, precision, recall = compute_metrics(preds.cpu(), mask_resized.cpu())\n",
    "        all_dice.append(dice)\n",
    "        all_iou.append(iou)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "        # Save first 20 predictions\n",
    "        if i < 20:\n",
    "            save_image(preds, os.path.join(save_pred_dir, f\"pred_{i}.png\"))\n",
    "            save_image(mask_resized.float(), os.path.join(save_pred_dir, f\"gt_{i}.png\"))\n",
    "\n",
    "# ---- FINAL RESULTS ----\n",
    "print(f\"[Test Results] Loss = {mean(test_losses):.4f}, Seg = {mean(test_seg_losses):.4f}, Dist = {mean(test_distil_losses):.4f}\")\n",
    "print(f\"[Metrics] mDice = {mean(all_dice):.4f}, mIoU = {mean(all_iou):.4f}, Precision = {mean(all_precision):.4f}, Recall = {mean(all_recall):.4f}\")\n",
    "print(f\"✅ Predictions saved in {save_pred_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]: 100%|███████████████████████████████████████████████| 10/10 [00:10<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Results] Loss = 0.1512, Seg = 0.2520, Dist = 0.0000\n",
      "[Metrics] mDice = 0.8416, mIoU = 0.7447, Precision = 0.8565, Recall = 0.8399\n",
      "✅ Predictions saved in /home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_cvc_300_unetplusplus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Testing for Kvasir with SAM + DINO + OneFormer Features ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from statistics import mean\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.clamp(min=1e-7, max=1 - 1e-7)\n",
    "        preds = preds.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        intersection = (preds * targets).sum(dim=(2, 3))\n",
    "        union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "def compute_metrics(preds, targets, smooth=1e-6):\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    tp = (preds * targets).sum().item()\n",
    "    fp = (preds * (1 - targets)).sum().item()\n",
    "    fn = ((1 - preds) * targets).sum().item()\n",
    "\n",
    "    dice = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
    "    iou = (tp + smooth) / (tp + fp + fn + smooth)\n",
    "    precision = (tp + smooth) / (tp + fp + smooth)\n",
    "    recall = (tp + smooth) / (tp + fn + smooth)\n",
    "\n",
    "    return dice, iou, precision, recall\n",
    "\n",
    "# ---- DEVICE ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- LOAD MODEL ----\n",
    "model = NestedUNet(num_classes=1, input_channels=3).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/home/deepak1010/Shivanshu Code/features_sam_clip/unetplusplus_sam_dino_oneformer.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "model.eval()\n",
    "\n",
    "# ---- LOSSES ----\n",
    "seg_loss = DiceLoss()\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---- TEST LOOP ----\n",
    "test_losses, test_seg_losses, test_distil_losses = [], [], []\n",
    "all_dice, all_iou, all_precision, all_recall = [], [], [], []\n",
    "\n",
    "save_pred_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_cvc_300_unetplusplus\"\n",
    "os.makedirs(save_pred_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tq = tqdm(test_cvc_300_dataloader, desc=\"[Test]\")\n",
    "    for i, batch in enumerate(tq):\n",
    "        # Input image & GT\n",
    "        x = batch[\"pixel_values\"].to(device)\n",
    "        mask = batch[\"ground_truth_mask\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # SAM features\n",
    "        f1 = batch[\"f1\"].to(device)                         # [B, N, C, H, W]\n",
    "        B, N, C, H, W = f1.shape\n",
    "        f1 = f1.view(B, N * C, H, W)                        # [B, N*C, H, W]\n",
    "\n",
    "        # DINOv2 features\n",
    "        f2 = batch[\"f2\"].to(device)                         # [B, 512, H, W]\n",
    "\n",
    "        # OneFormer features\n",
    "        f3 = batch[\"f3\"].to(device)                         # shape depends on extractor\n",
    "\n",
    "        # Forward pass\n",
    "        logits, dist1, dist2 = model(x, f1, f2, f3)\n",
    "\n",
    "        # Resize mask\n",
    "        mask_resized = nn.functional.interpolate(mask, size=logits.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Loss calculation (same as training)\n",
    "        seg_loss_value = seg_loss(logits, mask_resized) + criteria(logits, mask_resized)\n",
    "        loss = 0.6 * seg_loss_value + 0.1 * dist1 + 0.1 * dist2\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_seg_losses.append(seg_loss_value.item())\n",
    "        test_distil_losses.append((dist1.item() + dist2.item()) / 2)\n",
    "\n",
    "        # Predictions -> binary\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        # Metrics\n",
    "        dice, iou, precision, recall = compute_metrics(preds.cpu(), mask_resized.cpu())\n",
    "        all_dice.append(dice)\n",
    "        all_iou.append(iou)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "        # Save first 20 predictions\n",
    "        if i < 20:\n",
    "            save_image(preds, os.path.join(save_pred_dir, f\"pred_{i}.png\"))\n",
    "            save_image(mask_resized.float(), os.path.join(save_pred_dir, f\"gt_{i}.png\"))\n",
    "\n",
    "# ---- FINAL RESULTS ----\n",
    "print(f\"[Test Results] Loss = {mean(test_losses):.4f}, Seg = {mean(test_seg_losses):.4f}, Dist = {mean(test_distil_losses):.4f}\")\n",
    "print(f\"[Metrics] mDice = {mean(all_dice):.4f}, mIoU = {mean(all_iou):.4f}, Precision = {mean(all_precision):.4f}, Recall = {mean(all_recall):.4f}\")\n",
    "print(f\"✅ Predictions saved in {save_pred_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]: 100%|███████████████████████████████████████████████| 64/64 [01:12<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Results] Loss = 0.5373, Seg = 0.8955, Dist = 0.0000\n",
      "[Metrics] mDice = 0.7115, mIoU = 0.5987, Precision = 0.8333, Recall = 0.6787\n",
      "✅ Predictions saved in /home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_cvc_colondb_unetplusplus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Testing for Kvasir with SAM + DINO + OneFormer Features ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from statistics import mean\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.clamp(min=1e-7, max=1 - 1e-7)\n",
    "        preds = preds.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        intersection = (preds * targets).sum(dim=(2, 3))\n",
    "        union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "def compute_metrics(preds, targets, smooth=1e-6):\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    tp = (preds * targets).sum().item()\n",
    "    fp = (preds * (1 - targets)).sum().item()\n",
    "    fn = ((1 - preds) * targets).sum().item()\n",
    "\n",
    "    dice = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
    "    iou = (tp + smooth) / (tp + fp + fn + smooth)\n",
    "    precision = (tp + smooth) / (tp + fp + smooth)\n",
    "    recall = (tp + smooth) / (tp + fn + smooth)\n",
    "\n",
    "    return dice, iou, precision, recall\n",
    "\n",
    "# ---- DEVICE ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- LOAD MODEL ----\n",
    "model = NestedUNet(num_classes=1, input_channels=3).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/home/deepak1010/Shivanshu Code/features_sam_clip/unetplusplus_sam_dino_oneformer.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "model.eval()\n",
    "\n",
    "# ---- LOSSES ----\n",
    "seg_loss = DiceLoss()\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---- TEST LOOP ----\n",
    "test_losses, test_seg_losses, test_distil_losses = [], [], []\n",
    "all_dice, all_iou, all_precision, all_recall = [], [], [], []\n",
    "\n",
    "save_pred_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_cvc_colondb_unetplusplus\"\n",
    "os.makedirs(save_pred_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tq = tqdm(test_cvc_colondb_dataloader, desc=\"[Test]\")\n",
    "    for i, batch in enumerate(tq):\n",
    "        # Input image & GT\n",
    "        x = batch[\"pixel_values\"].to(device)\n",
    "        mask = batch[\"ground_truth_mask\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # SAM features\n",
    "        f1 = batch[\"f1\"].to(device)                         # [B, N, C, H, W]\n",
    "        B, N, C, H, W = f1.shape\n",
    "        f1 = f1.view(B, N * C, H, W)                        # [B, N*C, H, W]\n",
    "\n",
    "        # DINOv2 features\n",
    "        f2 = batch[\"f2\"].to(device)                         # [B, 512, H, W]\n",
    "\n",
    "        # OneFormer features\n",
    "        f3 = batch[\"f3\"].to(device)                         # shape depends on extractor\n",
    "\n",
    "        # Forward pass\n",
    "        logits, dist1, dist2 = model(x, f1, f2, f3)\n",
    "\n",
    "        # Resize mask\n",
    "        mask_resized = nn.functional.interpolate(mask, size=logits.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Loss calculation (same as training)\n",
    "        seg_loss_value = seg_loss(logits, mask_resized) + criteria(logits, mask_resized)\n",
    "        loss = 0.6 * seg_loss_value + 0.1 * dist1 + 0.1 * dist2\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_seg_losses.append(seg_loss_value.item())\n",
    "        test_distil_losses.append((dist1.item() + dist2.item()) / 2)\n",
    "\n",
    "        # Predictions -> binary\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        # Metrics\n",
    "        dice, iou, precision, recall = compute_metrics(preds.cpu(), mask_resized.cpu())\n",
    "        all_dice.append(dice)\n",
    "        all_iou.append(iou)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "        # Save first 20 predictions\n",
    "        if i < 20:\n",
    "            save_image(preds, os.path.join(save_pred_dir, f\"pred_{i}.png\"))\n",
    "            save_image(mask_resized.float(), os.path.join(save_pred_dir, f\"gt_{i}.png\"))\n",
    "\n",
    "# ---- FINAL RESULTS ----\n",
    "print(f\"[Test Results] Loss = {mean(test_losses):.4f}, Seg = {mean(test_seg_losses):.4f}, Dist = {mean(test_distil_losses):.4f}\")\n",
    "print(f\"[Metrics] mDice = {mean(all_dice):.4f}, mIoU = {mean(all_iou):.4f}, Precision = {mean(all_precision):.4f}, Recall = {mean(all_recall):.4f}\")\n",
    "print(f\"✅ Predictions saved in {save_pred_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Test]: 100%|███████████████████████████████████████████████| 33/33 [00:52<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Results] Loss = 0.4775, Seg = 0.7959, Dist = 0.0000\n",
      "[Metrics] mDice = 0.5188, mIoU = 0.3992, Precision = 0.6649, Recall = 0.5040\n",
      "✅ Predictions saved in /home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_etis_unetplusplus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Testing for Kvasir with SAM + DINO + OneFormer Features ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from statistics import mean\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.clamp(min=1e-7, max=1 - 1e-7)\n",
    "        preds = preds.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        intersection = (preds * targets).sum(dim=(2, 3))\n",
    "        union = preds.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "def compute_metrics(preds, targets, smooth=1e-6):\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    tp = (preds * targets).sum().item()\n",
    "    fp = (preds * (1 - targets)).sum().item()\n",
    "    fn = ((1 - preds) * targets).sum().item()\n",
    "\n",
    "    dice = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
    "    iou = (tp + smooth) / (tp + fp + fn + smooth)\n",
    "    precision = (tp + smooth) / (tp + fp + smooth)\n",
    "    recall = (tp + smooth) / (tp + fn + smooth)\n",
    "\n",
    "    return dice, iou, precision, recall\n",
    "\n",
    "# ---- DEVICE ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- LOAD MODEL ----\n",
    "model = NestedUNet(num_classes=1, input_channels=3).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/home/deepak1010/Shivanshu Code/features_sam_clip/unetplusplus_sam_dino_oneformer.pth\",\n",
    "    map_location=device\n",
    "))\n",
    "model.eval()\n",
    "\n",
    "# ---- LOSSES ----\n",
    "seg_loss = DiceLoss()\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---- TEST LOOP ----\n",
    "test_losses, test_seg_losses, test_distil_losses = [], [], []\n",
    "all_dice, all_iou, all_precision, all_recall = [], [], [], []\n",
    "\n",
    "save_pred_dir = \"/home/deepak1010/Shivanshu Code/features_sam_clip/test_predictions_etis_unetplusplus\"\n",
    "os.makedirs(save_pred_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tq = tqdm(test_etis_dataloader, desc=\"[Test]\")\n",
    "    for i, batch in enumerate(tq):\n",
    "        # Input image & GT\n",
    "        x = batch[\"pixel_values\"].to(device)\n",
    "        mask = batch[\"ground_truth_mask\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # SAM features\n",
    "        f1 = batch[\"f1\"].to(device)                         # [B, N, C, H, W]\n",
    "        B, N, C, H, W = f1.shape\n",
    "        f1 = f1.view(B, N * C, H, W)                        # [B, N*C, H, W]\n",
    "\n",
    "        # DINOv2 features\n",
    "        f2 = batch[\"f2\"].to(device)                         # [B, 512, H, W]\n",
    "\n",
    "        # OneFormer features\n",
    "        f3 = batch[\"f3\"].to(device)                         # shape depends on extractor\n",
    "\n",
    "        # Forward pass\n",
    "        logits, dist1, dist2 = model(x, f1, f2, f3)\n",
    "\n",
    "        # Resize mask\n",
    "        mask_resized = nn.functional.interpolate(mask, size=logits.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Loss calculation (same as training)\n",
    "        seg_loss_value = seg_loss(logits, mask_resized) + criteria(logits, mask_resized)\n",
    "        loss = 0.6 * seg_loss_value + 0.1 * dist1 + 0.1 * dist2\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_seg_losses.append(seg_loss_value.item())\n",
    "        test_distil_losses.append((dist1.item() + dist2.item()) / 2)\n",
    "\n",
    "        # Predictions -> binary\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        # Metrics\n",
    "        dice, iou, precision, recall = compute_metrics(preds.cpu(), mask_resized.cpu())\n",
    "        all_dice.append(dice)\n",
    "        all_iou.append(iou)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "\n",
    "        # Save first 20 predictions\n",
    "        if i < 20:\n",
    "            save_image(preds, os.path.join(save_pred_dir, f\"pred_{i}.png\"))\n",
    "            save_image(mask_resized.float(), os.path.join(save_pred_dir, f\"gt_{i}.png\"))\n",
    "\n",
    "# ---- FINAL RESULTS ----\n",
    "print(f\"[Test Results] Loss = {mean(test_losses):.4f}, Seg = {mean(test_seg_losses):.4f}, Dist = {mean(test_distil_losses):.4f}\")\n",
    "print(f\"[Metrics] mDice = {mean(all_dice):.4f}, mIoU = {mean(all_iou):.4f}, Precision = {mean(all_precision):.4f}, Recall = {mean(all_recall):.4f}\")\n",
    "print(f\"✅ Predictions saved in {save_pred_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 930614,
     "sourceId": 1574219,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1316959,
     "sourceId": 7681479,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4609976,
     "sourceId": 7859080,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4723588,
     "sourceId": 8017092,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8031061,
     "sourceId": 12707214,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4688791,
     "sourceId": 7968857,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
